Customize authorization for your storage bucket by defining access to file paths for guests, authenticated users, and user groups. Access can also be defined for functions that require access to the storage bucket.

To customize authorization, you need to have authentication set up. If you haven't already, set it up by following the documentation.

Note that paths in access definitions cannot have a '/' at the beginning of the string. By default, all paths are denied to all types of users unless explicitly granted.

There are several access types, including guest users, authenticated users, user groups, owners, and functions. 

To grant all guest users read access to files under a certain path, use the following access values:
```javascript
export const storage = defineStorage({
  name: 'myProjectFiles',
  access: (allow) => ({
    'media/*': [
      allow.guest.to(['read']) 
    ]
  })
});
```

To grant all authenticated users read access to files under a certain path, use the following access configuration:
```javascript
export const storage = defineStorage({
  name: 'myProjectFiles',
  access: (allow) => ({
    'media/*': [
      allow.authenticated.to(['read']) 
    ]
  })
});
```

When a user is part of a group, they are assigned the group role, which means permissions defined for the authenticated role will not apply for this user. To grant access to users within a group, you must explicitly define access permissions for the group against the desired prefix.

If you have configured user groups when setting up auth, you can scope storage access to specific groups. For example, assume you have a `defineAuth` config with `admin` and `auditor` groups. With the following access definition, you can configure permissions such that auditors have read-only permissions to a certain path while admin has full permissions.
```javascript
export const storage = defineStorage({
  name: 'myProjectFiles',
  access: (allow) => ({
    'media/*': [
      allow.groups(['auditor']).to(['read']),
      allow.groups(['admin']).to(['read', 'write', 'delete'])
    ]
  })
});
```

In some use cases, you may want just the uploader of a file to be able to perform actions on it. You can do this by using the `entity_id` to represent the user which scopes files to individual users.

The `entity_id` is a reserved token that will be replaced with the users' identifier when the file is being uploaded. You can specify the method of identification when defining access to the path like `allow.entity(<identification_method>).to([..])`. 

For example, the following policy would allow authenticated users full access to a certain path that matches their identity id.
```javascript
export const storage = defineStorage({
  name: 'myProjectFiles',
  access: (allow) => ({
    'media/{entity_id}/*': [
      allow.entity('identity').to(['read', 'write', 'delete'])
    ]
  })
});
```

In addition to granting application users access to storage files, you may also want to grant a backend function access to storage files. This could be used to enable a use case like resizing images or automatically deleting old files.

For example, the following configuration is used to define function access.
```javascript
import { defineStorage, defineFunction } from '@aws-amplify/backend';

const demoFunction = defineFunction({});

export const storage = defineStorage({
  name: 'myProjectFiles',
  access: (allow) => ({
    'media/*': [allow.resource(demoFunction).to(['read', 'write', 'delete'])]
  })
});
```

There are some rules for the types of paths that can be specified at the same time in the storage access definition. All paths must end with `/*`. Only one level of nesting is allowed. Wildcards cannot conflict with the `{entity_id}` token. A path cannot be a prefix of another path with an `{entity_id}` token.

When one path is a subpath of another, the permissions on the subpath always override the permissions from the parent path. Permissions are not "inherited" from a parent path.

When you configure access to a particular path, you can scope the access to one or more CRUDL actions. The available actions are `read`, `get`, `list`, `write`, and `delete`. Note that `read` is a combination of `get` and `list` access definitions and hence cannot be defined in the presence of `get` or `list`.

To configure `defineStorage` in Amplify Gen 2 to behave the same way as the storage category in Gen 1, you can use the following definition.
```javascript
export const storage = defineStorage({
  name: 'myProjectFiles',
  access: (allow) => ({
    'public/*': [
      allow.guest.to(['read']),
      allow.authenticated.to(['read', 'write', 'delete']),
    ],
    'protected/{entity_id}/*': [
      allow.authenticated.to(['read']),
      allow.entity('identity').to(['read', 'write', 'delete'])
    ],
    'private/{entity_id}/*': [
      allow.entity('identity').to(['read', 'write', 'delete'])
    ]
  })
});
```

You can copy a file in Amplify Storage using the `copy` method. This method duplicates an existing file to a designated path and returns an object with the path of the copied file upon successful completion.

```javascript
import { copy } from 'aws-amplify/storage';

const copyFile = async () => {
  try {
    const response = await copy({
      source: {
        path: `album/2024/${encodeURIComponent('#1.jpg')}`,
      },
      destination: {
        path: 'shared/2024/#1.jpg',
      },
    });
  } catch (error) {
    console.error('Error', error);
  }
};
```

Note that you can only copy files up to 5GB in a single operation. Also, if there's a special character in the source path, you should URI encode the source path.

You can also specify a bucket or copy across buckets/regions by providing the `bucket` option. This option can be a string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.

```javascript
import { copy } from 'aws-amplify/storage';

const copyFile = async () => {
  try {
    const response = await copy({
      source: {
        path: 'album/2024/1.jpg',
        bucket: 'assignedNameInAmplifyBackend',
        expectedBucketOwner: '123456789012'
      },
      destination: {
        path: 'shared/2024/1.jpg',
        bucket: {
          bucketName: 'generated-second-bucket-name',
          region: 'us-east-2'
        },
        expectedBucketOwner: '123456789013'
      }
    });
  } catch (error) {
    console.error('Error', error);
  }
};
```

To copy to or from a bucket other than your default, both source and destination must have `bucket` explicitly defined.

The `copy` method takes several options, including:

* `path`: a string or callback that represents the path in the source and destination bucket to copy the object to or from
* `bucket`: a string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console
* `eTag`: the copy source object entity tag (ETag) value
* `notModifiedSince`: copies the source object if it hasn't been modified since the specified time
* `expectedBucketOwner`: the account ID that owns the source or destination bucket

Cross-identity ID copying is only allowed if the destination path has the right access rules to allow other authenticated users writing to it.

In React, you can use the `copy` method as shown above to copy files in Amplify Storage. Make sure to handle errors and exceptions properly, and use the `bucket` option to specify the target bucket if necessary.

When submitting an app to the App Store, Apple requires developers to disclose the data usage policy of their app. The Amplify Library, used to interact with AWS resources, collects certain data types that need to be disclosed in the app's data usage policy.

The Amplify Library gathers API usage metrics from the AWS services accessed, which involves adding a user agent to the request made to the AWS service. The user-agent header includes information about the Amplify Library version, operating system name, and version. This data is collected to generate metrics related to library usage and is not linked to the user's identity or used for tracking purposes.

The Amplify Library collects various data types, including:

Contact Info:
- Name
- Email Address
- Phone Number

These data types are used for app functionality, such as authentication, and are linked to the user's identity but not used for tracking purposes.

User Content:
- Photos or Videos
- Audio Data

These data types are used for app functionality, such as storage and predictions, and are not linked to the user's identity or used for tracking purposes.

Identifiers:
- User ID
- Device ID

These data types are used for app functionality, such as authentication and analytics, and are linked to the user's identity but not used for tracking purposes.

Other Data:
- OS Version
- OS Name
- Locale Info
- App Version
- Min OS target of the app
- Timezone information
- Network information
- Has SIM card
- Cellular Carrier Name
- Device Model
- Device Name
- Device OS Version
- Device Height and Width
- Device Language
- identifierForVendor

These data types are used for analytics and app functionality and are not linked to the user's identity or used for tracking purposes.

The Amplify Library does not collect data related to:
- Health and Fitness
- Financial Info
- Location
- Sensitive Info
- Contacts
- Browsing History
- Search History
- Diagnostics

Some Amplify categories, such as Analytics, Auth, and DataStore, persist data to the local device. This data is automatically removed when a user uninstalls the app from the device. However, Auth information is stored in the local system keychain, which does not guarantee removal when an app is uninstalled. To clear this data, app developers should decide when to clear the data by signing out, for example, by using `Auth.signOut()` when the app is launched for the first time. 

For example in a React application using Amplify you could clear the Auth data as follows:
```javascript
import Amplify from 'aws-amplify';
import Auth from '@aws-amplify/auth';

// Check if the app is launching for the first time
// and sign out if it is
if (/* check if app is launching for the first time */) {
  Auth.signOut()
   .then(data => console.log(data))
   .catch(err => console.log(err));
}
```

You can easily display images in your app by using the cloud-connected Storage Image React UI component. This component fetches images securely from your storage resource and displays it on the web page.

To get started, you need to install the required packages by running the command `npm add @aws-amplify/ui-react-storage aws-amplify` in your terminal.

Here's an example of how to use the Storage Image component:
```tsx
import { StorageImage } from '@aws-amplify/ui-react-storage';

export const DefaultStorageImageExample = () => {
  return <StorageImage alt="cat" path="your-path/cat.jpg" />;
};
```
You can further customize the UI component by referring to the [Storage Image documentation](https://ui.docs.amplify.aws/react/connected-components/storage/storageimage).

To download files from your storage, you can use the `getUrl` API from the Amplify Library for Storage. This API generates a presigned URL that is valid for 900 seconds or 15 minutes by default. You can use this URL to create a download link for users to click on.

Here's an example of how to use the `getUrl` API:
```javascript
import { getUrl } from 'aws-amplify/storage';

const linkToStorageFile = await getUrl({
  path: "album/2024/1.jpg",
});
console.log('signed URL: ', linkToStorageFile.url);
console.log('URL expires at: ', linkToStorageFile.expiresAt);
```
You can then use the `url` property to create a link to the file:
```tsx
<a href={linkToStorageFile.url.toString()} target="_blank" rel="noreferrer">
  {fileName} 
</a>
```
Note that the `getUrl` API does not check if the file exists by default. As a result, the signed URL may fail if the file to be downloaded does not exist.

You can also customize the behavior of the `getUrl` API by passing in options. For example, you can specify a target bucket using the `bucket` option, or ensure that the object exists before getting the URL using the `validateObjectExistence` option.

Here's an example of how to use the `getUrl` API with options:
```typescript
import { getUrl } from 'aws-amplify/storage';

const linkToStorageFile = await getUrl({
  path: "album/2024/1.jpg",
  options: {
    bucket: 'assignedNameInAmplifyBackend',
    validateObjectExistence: true,
    expiresIn: 300,
    useAccelerateEndpoint: true,
    expectedBucketOwner: '123456789012',
  }
});
```
The available options for the `getUrl` API are:

* `bucket`: A string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.
* `validateObjectExistence`: A boolean indicating whether to head object to make sure the object existence before downloading.
* `expiresIn`: A number representing the number of seconds till the URL expires.
* `useAccelerateEndpoint`: A boolean indicating whether to use accelerate endpoint.
* `expectedBucketOwner`: A string representing the account ID that owns the requested bucket.

To download a file locally, you can use the `downloadData` API from the Amplify Library for Storage. This API downloads the file content to memory.

Here's an example of how to use the `downloadData` API:
```javascript
import { downloadData } from 'aws-amplify/storage';

const { body, eTag } = await downloadData({
  path: "album/2024/1.jpg"
}).result;
```
You can then get the text value of the downloaded file using the `text()` method:
```javascript
const text = await body.text();
console.log('Succeed: ', text);
```
You can also download a file from a specified bucket by providing the `bucket` option. You can pass in a string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.

Here's an example of how to use the `downloadData` API with a specified bucket:
```typescript
import { downloadData } from 'aws-amplify/storage';

const result = await downloadData({
  path: 'album/2024/1.jpg',
  options: {
    bucket: 'assignedNameInAmplifyBackend'
  }
}).result;
```
Alternatively, you can also pass in an object by specifying the bucket name and region from the console:
```typescript
import { downloadData } from 'aws-amplify/storage';

const result = await downloadData({
  path: 'album/2024/1.jpg',
  options: {
    bucket: {
      bucketName: 'bucket-name-from-console',
      region: 'us-east-2'
    }
  }
}).result;
```
You can monitor the download progress by using the `onProgress` option:
```javascript
import { downloadData } from 'aws-amplify/storage';

const { body, eTag } = await downloadData({
  path: "album/2024/1.jpg",
  options: {
    onProgress: (progress) => {
      console.log(`Download progress: ${(progress.transferredBytes/progress.totalBytes) * 100}%`);
    }
  }
}).result;
```
You can also cancel the download operation using the `cancel` method:
```javascript
import { downloadData, isCancelError } from 'aws-amplify/storage';

const downloadTask = downloadData({ path: 'album/2024/1.jpg' });
downloadTask.cancel();
try {
  await downloadTask.result;
} catch (error) {
  if (isCancelError(error)) {
    console.log('Download operation cancelled');
  }
}
```
The available options for the `downloadData` API are:

* `bucket`: A string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.
* `onProgress`: A callback function tracking the upload/download progress.
* `bytesRange`: An object specifying the bytes range to download.
* `useAccelerateEndpoint`: A boolean indicating whether to use accelerate endpoint.
* `expectedBucketOwner`: A string representing the account ID that owns the requested bucket.

You can also frequently asked questions about the Storage API, such as:

* `downloadData` is cached; if you have recently modified a file you may not get the latest version right away. You can pass in `cacheControl: 'no-cache'` to get the latest version.
* `downloadData` only returns the latest cached version of the file; there is not yet an API to view prior versions.
* Image compression or CloudFront CDN caching for your S3 buckets is not yet possible.
* There is no API for Cognito Group-based access to files.
* There is currently no API for getting the `identityId` of other users; you have to retrieve this from elsewhere before calling `Storage.get`.

For Amplify-generated S3 resources, you can access the underlying Amazon S3 resources to customize your backend configuration using the AWS Cloud Developer Kit (AWS CDK). To enable Transfer Acceleration on the bucket, you need to unwrap the L1 CDK construct from the L2 CDK construct. Here is an example of how to do this in React:

```tsx
import * as s3 from 'aws-cdk-lib/aws-s3';
import { defineBackend } from '@aws-amplify/backend';
import { storage } from './storage/resource';

const backend = defineBackend({
  storage
});

const s3Bucket = backend.storage.resources.bucket;
const cfnBucket = s3Bucket.node.defaultChild as s3.CfnBucket;

cfnBucket.accelerateConfiguration = {
  accelerationStatus: "Enabled"
}
```

To upload files using the accelerated S3 endpoint, you can set the `useAccelerateEndpoint` parameter to `true` in the `AWSS3StorageUploadFileOptions`. However, since you are using React, you would use the `Amplify.Storage` API to upload files. Here is an example of how to upload a file using the accelerated S3 endpoint in React:

```tsx
import Amplify from 'aws-amplify';
import { Storage } from '@aws-amplify/storage';

// Initialize Amplify
Amplify.configure({
  // Your Amplify configuration
});

// Upload a file using the accelerated S3 endpoint
const file = new File(['file content'], 'example.txt', {
  type: 'text/plain',
});
const key = 'public/example';
const options = {
  useAccelerateEndpoint: true,
};

Storage.put(key, file, options)
 .then((result) => console.log(result))
 .catch((error) => console.error(error));
```

For manually configured S3 resources, you need to set up a CORS Policy for your S3 bucket to make calls to your S3 bucket from your app. You can do this by following these steps:

1. Go to the Amazon S3 console and click on your project's `userfiles` bucket.
2. Click on the **Permissions** tab for your bucket.
3. Click the edit button in the **Cross-origin resource sharing (CORS)** section.
4. Make the changes and click on Save Changes. You can add required metadata to be exposed in `ExposeHeaders` with `x-amz-meta-XXXX` format.

Here is an example of a CORS configuration:

```json
[
  {
    "AllowedHeaders": ["*"],
    "AllowedMethods": ["GET", "HEAD", "PUT", "POST", "DELETE"],
    "AllowedOrigins": ["*"],
    "ExposeHeaders": [
      "x-amz-server-side-encryption",
      "x-amz-request-id",
      "x-amz-id-2",
      "ETag",
      "x-amz-meta-foo"
    ],
    "MaxAgeSeconds": 3000
  }
]
```

Note that you can restrict access to your bucket by updating `AllowedOrigin` to include individual domains.

To listen to storage events in React, you can configure function triggers to enable event-based workflows when files are uploaded or deleted. This is achieved by modifying the defineStorage configuration in your storage definition.

First, add the following code to your storage definition to configure triggers for upload and delete events:

```javascript
const storage = defineStorage({
  name: 'myProjectFiles',
  triggers: {
    onUpload: defineFunction({
      entry: './on-upload-handler.ts'
    }),
    onDelete: defineFunction({
      entry: './on-delete-handler.ts'
    })
  }
});
```

Then, create function definitions for the upload and delete handlers. These handlers will be invoked when an object is uploaded or deleted from the bucket.

```javascript
// on-upload-handler.ts
import type { S3Handler } from 'aws-lambda';

export const handler: S3Handler = async (event) => {
  const objectKeys = event.Records.map((record) => record.s3.object.key);
  console.log(`Upload handler invoked for objects [${objectKeys.join(', ')}]`);
};
```

```javascript
// on-delete-handler.ts
import type { S3Handler } from 'aws-lambda';

export const handler: S3Handler = async (event) => {
  const objectKeys = event.Records.map((record) => record.s3.object.key);
  console.log(`Delete handler invoked for objects [${objectKeys.join(', ')}]`);
};
```

These handlers will be invoked whenever an object is uploaded or deleted from the bucket.

If you need more advanced triggers, you can use the addEventNotification method in your backend configuration. This allows you to define more complex event notifications, such as triggering a Lambda function when a file with a specific prefix and suffix is uploaded.

```javascript
// backend.ts
import { EventType } from 'aws-cdk-lib/aws-s3';
import { LambdaDestination } from 'aws-cdk-lib/aws-s3-notifications';
import { defineBackend } from '@aws-amplify/backend';
import { storage } from './storage/resource';
import { yourLambda } from './functions/yourLambda/resource';

const backend = defineBackend({
  storage,
  yourLambda,
});

backend.storage.resources.bucket.addEventNotification(
  EventType.OBJECT_CREATED_PUT,
  new LambdaDestination(backend.yourLambda.resources.lambda),
  {
    prefix: 'protected/uploads/',
    suffix: '-uploadManifest.json',
  }
);
```

This modification creates a new AWS CloudFormation handler that specifically handles checking the prefix and suffix, and triggers the Lambda function accordingly.

You can list files without having to download all the files by using the list API from the Amplify Library for Storage. You can also get properties individually for a file using the getProperties API.

## List Files

To list files, you can use the list function from the Amplify Storage library. This function returns a list of files in the specified path. 

```javascript
import { list } from 'aws-amplify/storage';

const result = await list({
  path: 'album/photos/',
});
```

Note the trailing slash `/` in the path. If you had requested `list({ path :  'album/photos' })` it would also match against files like `album/photos123.jpg` alongside `album/photos/123.jpg`.

The format of the response will look similar to the below example:

```js
{
  items: [
    {
      path: "album/photos/123.jpg",
      eTag: "30074401292215403a42b0739f3b5262",
      lastModified: "Thu Oct 08 2020 23:59:31 GMT+0800 (Singapore Standard Time)",
      size: 138256
    },
    //...
  ],
}
```

If the pageSize is set lower than the total file size, a single list call only returns a subset of all the files. To list all the files with multiple calls, users can use the nextToken flag:

```javascript
import { list } from 'aws-amplify/storage';

const PAGE_SIZE = 20;
let nextToken;
const loadNextPage = async () => {
  const response = await list({
    path: 'photos/',
    options: {
      pageSize: PAGE_SIZE,
      nextToken,
    },
  });
  if (response.nextToken) {
    nextToken = response.nextToken;
  } else {
    nextToken = undefined;
  }
  // render list items from response.items
};
```

### List All files

You can also list all files by setting the listAll option to true:

```javascript
import { list } from 'aws-amplify/storage';

const result = await list({
  path: 'album/photos/',
  options: {
    listAll: true,
  }
});
```

Manually created folders will show up as files with a size of 0. Since "folders" are a virtual concept in Amazon S3, any file may declare any depth of folder just by having a `/` in its name.

To access the contents and subpaths of a "folder", you have two options:

1. Request the entire path and parse the contents.
2. Use the subpathStrategy option to retrieve only the files within the specified path (i.e. exclude files under subpaths).

### Get all nested files within a path

This retrieves all files and folders under a given path. You may need to parse the result to get only the files within the specified path.

```javascript
function processStorageList(response) {
  let files = [];
  let folders = new Set();
  response.items.forEach((res) => {
    if (res.size) {
      files.push(res);
      // sometimes files declare a folder with a / within then
      let possibleFolder = res.path.split('/').slice(0, -1).join('/');
      if (possibleFolder) folders.add(possibleFolder);
    } else {
      folders.add(res.path);
    }
  });
  return { files, folders };
}
```

If you need the files and folders in terms of a nested object instead (for example, to build an explorer UI), you could parse it recursively:

```javascript
function processStorageList(response) {
  const filesystem = {};
  // https://stackoverflow.com/questions/44759750/how-can-i-create-a-nested-object-representation-of-a-folder-structure
  const add = (source, target, item) => {
    const elements = source.split('/');
    const element = elements.shift();
    if (!element) return; // blank
    target[element] = target[element] || { __data: item }; // element;
    if (elements.length) {
      target[element] =
        typeof target[element] === 'object'? target[element] : {};
      add(elements.join('/'), target[element], item);
    }
  };
  response.items.forEach((item) => add(item.path, filesystem, item));
  return filesystem;
}
```

This places each item's data inside a special `__data` key.

### Excluding subpaths

In addition to using the list API to get all the contents of a path, you can also use it to get only the files within a path while excluding files under subpaths.

For example, given the following keys in your path you may want to return only the jpg object, and not the "vacation" subpath and its contents:

```
photos/photo1.jpg
photos/vacation/
```

This can be accomplished with the subpathStrategy option:

```javascript
import { list } from "aws-amplify/storage";
const result = await list({ 
  path: "photos/",
  options:{
    subpathStrategy: { strategy:'exclude' }
  }
});
```

The response will include only the objects within the photos/ path and will also communicate any excluded subpaths:

```js
{
    excludedSubpaths: [
      'photos/vacation/'
    ],
    items: [
      {
        path: "photos/photo1.jpg",
        eTag: "30074401292215403a42b0739f3b5262",
        lastModified: "Thu Oct 08 2020 23:59:31 GMT+0800 (Singapore Standard Time)",
        size: 138256
      },
    ]
}
```

The default delimiter character is '/', but this can be changed by supplying a custom delimiter:

```javascript
const result = await list({
  // Path uses '-' character to organize files rather than '/'
  path: 'photos-',
  options: {
    subpathStrategy: {
      strategy: 'exclude',
      delimiter: '-'
    }
  }
});
```

### List files from a specified bucket

You can also perform a list operation to a specific bucket by providing the bucket option. This option can either be a string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.

```javascript
import { list } from 'aws-amplify/storage';

const result = await list({
  path: 'photos/',
  options: {
    // Specify a target bucket using name assigned in Amplify Backend
    bucket: 'assignedNameInAmplifyBackend',
    // Alternatively, provide bucket name from console and associated region
    // bucket: {
    //   bucketName: 'generated-secondary-bucket-name',
    //   region: 'us-east-2'
    // }
  }
});
```

### More list options

| Option | Type | Default | Description |
| -- | :--: | :--: | ----------- |
| bucket | string \| <br />\{ bucketName: string;<br/> region: string; \} | Default bucket and region from Amplify configuration | A string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.<br/><br/>Read more at [Configure additional storage buckets](/[platform]/build-a-backend/storage/set-up-storage/#configure-additional-storage-buckets) |
| listAll | boolean | false | Set to true to list all files within the specified path |
| pageSize | number | 1000 | Sets the maximum number of files to be return. The range is 0 - 1000 |
| nextToken | string | â€” | Indicates whether the list is being continued on this bucket with a token |
| subpathStrategy | \{ strategy: 'include' \} \|<br/>\{ 'exclude',<br />delimiter?: string \} | \{ strategy: 'include' \} | An object representing the subpath inclusion strategy and the delimiter used to group results for exclusion. <br/><br/> Read more at [Excluding subpaths](/[platform]/build-a-backend/storage/list-files/#excluding-subpaths) |
| useAccelerateEndpoint | boolean | false | Whether to use accelerate endpoint. <br/><br/> Read more at [Transfer Acceleration](/[platform]/build-a-backend/storage/extend-s3-resources/#example---enable-transfer-acceleration) |
| expectedBucketOwner | string | Optional | The account ID that owns requested bucket. |

## Get File Properties

You can also view the properties of an individual file.

```javascript
import { getProperties } from 'aws-amplify/storage';

try {
  const result = await getProperties({
    path: 'album/2024/1.jpg',
    options: {
      // Specify a target bucket using name assigned in Amplify Backend
      bucket: 'assignedNameInAmplifyBackend'
    }
  });
  console.log('File Properties ', result);
} catch (error) {
  console.log('Error ', error);
}
```

The properties and metadata will look similar to the below example

```js
{
  path: "album/2024/1.jpg",
  contentType: "image/jpeg",
  contentLength: 6873,
  eTag: "\"56b32cf4779ff6ca3ba3f2d455fa56a7\"",
  lastModified: Wed Apr 19 2023 14:20:55 GMT-0700 (Pacific Daylight Time) {},
  metadata: { owner: 'aws' }
}
```

### More getProperties options

Option | Type | Default | Description |
| -- | -- | -- | ----------- |
| bucket | string \| <br />\{ bucketName: string;<br/> region: string; \} | Default bucket and region from Amplify configuration | A string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.<br/><br/>Read more at [Configure additional storage buckets](/[platform]/build-a-backend/storage/set-up-storage/#configure-additional-storage-buckets) |
| useAccelerateEndpoint | boolean | false | Whether to use accelerate endpoint. | [Transfer Acceleration](/[platform]/build-a-backend/storage/extend-s3-resources/#example---enable-transfer-acceleration) |

To get the metadata in result for all APIs you have to configure user defined metadata in CORS.

Learn more about how to setup an appropriate [CORS Policy](/[platform]/build-a-backend/storage/extend-s3-resources/#for-manually-configured-s3-resources).

The File storage page in the Amplify console provides a user-friendly interface for managing your application's backend file storage, allowing for efficient testing and management of your files.

If you haven't created a storage resource, you can visit the Storage setup guide to get started.

To access the File storage manager, follow these steps:
1. Log in to the Amplify console and choose your app.
2. Select the branch you want to access.
3. Select Storage from the left navigation bar.

To upload a file, you can either:
- Select the Upload button, choose the file you want to upload, and then select Done
- Drag and drop a file onto the Storage page

To delete a file:
1. Select the file you want to delete on the Storage page.
2. Select the Actions dropdown and then select Delete.

To copy a file:
1. Select the file you want to copy on the Storage page.
2. Select the Actions dropdown and then select Copy to.
3. Choose or create the folder where you want to save a copy of your file.
4. Select Copy to copy your file to the selected folder.

To move a file:
1. Select the file you want to move on the Storage page.
2. Select the Actions dropdown and then select Move to.
3. Choose or create the folder where you want to move your file.
4. Select Move to move your file to the selected folder.

Files can be removed from a storage bucket using the remove API. If a file is protected by an identity Id, only the user who owns the file will be able to remove it.

You can perform a remove operation from a specific bucket by providing the target bucket's assigned name from Amplify Backend in the bucket option. 

```javascript
import { remove } from 'aws-amplify/storage';

try {
  await remove({ 
    path: 'album/2024/1.jpg',
    bucket: 'assignedNameInAmplifyBackend', 
  });
} catch (error) {
  console.log('Error ', error);
}
```

Alternatively, you can also pass in an object by specifying the bucket name and region from the console.

```javascript
import { remove } from 'aws-amplify/storage';

try {
  await remove({ 
    path: 'album/2024/1.jpg',
    bucket: {
      bucketName: 'bucket-name-from-console',
      region: 'us-east-2'
    }
  });
} catch (error) {
  console.log('Error ', error);
}
```

There are additional options that can be used with the remove API. 

Option | Type | Default | Description 
| -- | :--: | :--: | ----------- 
| bucket | string or { bucketName: string; region: string; } | Default bucket and region from Amplify configuration | A string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.
| expectedBucketOwner | string | Optional | The account ID that owns requested bucket. 

Note: The account ID of the owner of the bucket is required when removing objects from a bucket that is not owned by the account that is making the request.

In this guide, you will learn how to set up storage in your Amplify app. You will set up your backend resources, and enable listing, uploading, and downloading files.

If you have not yet created an Amplify app, visit the quickstart guide.

Amplify Storage seamlessly integrates file storage and management capabilities into frontend web and mobile apps, built on top of Amazon Simple Storage Service (Amazon S3). It provides intuitive APIs and UI components for core file operations, enabling developers to build scalable and secure file storage solutions without dealing with cloud service complexities.

First, create a file `amplify/storage/resource.ts`. This file will be the location where you configure your storage backend. Instantiate storage using the `defineStorage` function and providing a `name` for your storage bucket. This `name` is a friendly name to identify your bucket in your backend configuration. Amplify will generate a unique identifier for your app using a UUID, the name attribute is just for use in your app.

```javascript
export const storage = defineStorage({
  name: 'amplifyTeamDrive'
});
```

Import your storage definition in your `amplify/backend.ts` file that contains your backend definition. Add storage to `defineBackend`.

```javascript
import { defineBackend } from '@aws-amplify/backend';
import { auth } from './auth/resource';
import { storage } from './storage/resource';

defineBackend({
  auth,
  storage
});
```

Now when you run `npx ampx sandbox` or deploy your app on Amplify, it will configure an Amazon S3 bucket where your files will be stored. Before files can be accessed in your application, you must configure storage access rules.

To deploy these changes, commit them to git and push the changes upstream. Amplify's CI/CD system will automatically pick up the changes and build and deploy the updates.

```bash
git commit -am "add storage backend"
git push
```

By default, no users or other project resources have access to any files in the storage bucket. Access must be explicitly granted within `defineStorage` using the `access` callback.

The access callback returns an object where each key in the object is a file path and each value in the object is an array of access rules that apply to that path.

For example, you can set up your file storage structure for a generic photo sharing app. Here, guests have access to see all profile pictures and only the users that uploaded the profile picture can replace or delete them. Users are identified by their Identity Pool ID in this case i.e. identityID. There's also a general pool where all users can submit pictures.

```javascript
export const storage = defineStorage({
  name: 'amplifyTeamDrive',
  access: (allow) => ({
    'profile-pictures/{entity_id}/*': [
      allow.guest.to(['read']),
      allow.entity('identity').to(['read', 'write', 'delete'])
    ],
    'picture-submissions/*': [
      allow.authenticated.to(['read','write']),
      allow.guest.to(['read', 'write'])
    ],
  })
});
```

Amplify Storage gives you the flexibility to configure your backend to automatically provision and manage multiple storage resources. You can define additional storage buckets by using the same `defineStorage` function and providing a unique, descriptive `name` to identify the storage bucket. You can pass this `name` to the storage APIs to specify the bucket you want to perform the action to. Ensure that this `name` attribute is unique across the defined storage buckets in order to reliably identify the correct bucket and prevent conflicts.

It's essential to note that if additional storage buckets are defined, one of them must be marked as default with the `isDefault` flag.

```javascript
export const firstBucket = defineStorage({
  name: 'firstBucket',
  isDefault: true, 
});

export const secondBucket = defineStorage({
  name: 'secondBucket',
  access: (allow) => ({
    'private/{entity_id}/*': [
      allow.entity('identity').to(['read', 'write', 'delete'])
    ]
  })
})
```

Add additional storage resources to the backend definition.

```javascript
import { defineBackend } from '@aws-amplify/backend';
import { auth } from './auth/resource';
import { firstBucket, secondBucket } from './storage/resource';

defineBackend({
  auth,
  firstBucket,
  secondBucket
});
```

Additional storage buckets can be referenced from application code by passing the `bucket` option to Amplify Storage APIs. You can provide a target bucket's name assigned in Amplify Backend.

```javascript
import { downloadData } from 'aws-amplify/storage';

try {
  const result = downloadData({
    path: "album/2024/1.jpg",
    options: {
      bucket: "secondBucket"
    }
  }).result;
} catch (error) {
  console.log(`Error: ${error}`)
}
```

Alternatively, you can also pass in an object by specifying the bucket name and region from the console.

```javascript
import { downloadData } from 'aws-amplify/storage';

try {
  const result = downloadData({
    path: 'album/2024/1.jpg',
    options: {
      bucket: {
        bucketName: 'second-bucket-name-from-console',
        region: 'us-east-2'
      }
    }
  }).result;
} catch (error) {
  console.log(`Error: ${error}`);
}
```

To connect your app code to the storage backend, import and load the configuration file in your app. Initialize the Amplify Storage category by calling `Amplify.add(plugin:)`. To complete initialization, call `Amplify.configure()`.

```javascript
import { Amplify } from 'aws-amplify';
import outputs from '../amplify_outputs.json';

Amplify.configure(outputs);
```

To upload your first file, you can use the `uploadData` function from `aws-amplify/storage`.

```jsx
import React from 'react';
import { uploadData } from 'aws-amplify/storage';

function App() {
  const [file, setFile] = React.useState();

  const handleChange = (event) => {
    setFile(event.target.files?.[0]);
  };

  const handleClick = () => {
    if (!file) {
      return;
    }
    uploadData({
      path: `picture-submissions/${file.name}`,
      data: file,
    });
  };

  return (
    <div>
      <input type="file" onChange={handleChange} />
      <button onClick={handleClick}>Upload</button>
    </div>
  );
}
```

After successfully publishing your storage backend and connecting your project with client APIs, you can manage files and folders in the Amplify console. You can perform on-demand actions like upload, download, copy, and more under the Storage tab in the console.

Congratulations! You finished the Set up Amplify Storage guide. In this guide, you set up and connected to backend resources, customized your file paths and access definitions, and connected your application to the backend to implement features like file uploads and downloads.

Now that you have completed setting up storage in your Amplify app, you can proceed to add file management features to your app. You can use the following guides to implement upload and download functionality, or you can access more capabilities from the side navigation.

- Upload Files
- Download Files

You can implement upload functionality in your app by either using the File Uploader UI component or further customizing the upload experience using the upload API.

To use the File Uploader UI component, you need to install the required packages by running the command `npm add @aws-amplify/ui-react-storage aws-amplify` in your terminal. Then, you can use the component in your app like this:

```tsx
import { FileUploader } from '@aws-amplify/ui-react-storage';
import '@aws-amplify/ui-react/styles.css';

export const DefaultFileUploaderExample = () => {
  return (
    <FileUploader
      acceptedFileTypes={['image/*']}
      path="public/"
      maxFileCount={1}
      isResumable
    />
  );
};
```

To implement upload functionality using the upload API, you can use the `uploadData` function from the `aws-amplify/storage` module. Here's an example of how to upload a file from a file object:

```jsx
import React from 'react';
import { uploadData } from 'aws-amplify/storage';

function App() {
  const [file, setFile] = React.useState();

  const handleChange = (event) => {
    setFile(event.target.files?.[0]);
  };

  const handleClick = () => {
    if (!file) {
      return;
    }
    uploadData({
      path: `photos/${file.name}`,
      data: file,
    });
  };

  return (
    <div>
      <input type="file" onChange={handleChange} />
      <button onClick={handleClick}>Upload</button>
    </div>
  );
}
```

You can also upload data from a data object. Here's an example:

```javascript
import { uploadData } from 'aws-amplify/storage';

try {
  const result = await uploadData({
    path: "album/2024/1.jpg",
    data: file,
  }).result;
  console.log('Succeeded: ', result);
} catch (error) {
  console.log('Error : ', error);
}
```

Additionally, you can monitor the progress of an upload by using the `onProgress` option. Here's an example:

```javascript
import { uploadData } from 'aws-amplify/storage';

const monitorUpload = async () => {
  try {
    const result = await uploadData({
      path: "album/2024/1.jpg",
      data: file,
      options: {
        onProgress: ({ transferredBytes, totalBytes }) => {
          if (totalBytes) {
            console.log(
              `Upload progress ${Math.round(
                (transferredBytes / totalBytes) * 100
              )} %`
            );
          }
        },
      },
    }).result;
    console.log("Path from Response: ", result.path);
  } catch (error) {
    console.log("Error : ", error);
  }
}
```

You can also pause, resume, and cancel uploads using the `pause`, `resume`, and `cancel` methods.

```javascript
import { uploadData, isCancelError } from 'aws-amplify/storage';

const uploadTask = uploadData({ path, data: file });
uploadTask.pause();
uploadTask.resume();
uploadTask.cancel();
try {
  await uploadTask.result;
} catch (error) {
  if (isCancelError(error)) {
    console.log("Upload was cancelled");
  }
}
```

You can customize the behavior of `uploadData` and the properties of the uploaded object by passing in additional options. Here are some of the available options:

* `bucket`: a string representing the target bucket's assigned name in Amplify Backend or an object specifying the bucket name and region from the console.
* `contentType`: the default content-type header value of the file when downloading it.
* `contentEncoding`: the default content-encoding header value of the file when downloading it.
* `contentDisposition`: specifies presentational information for the object.
* `metadata`: a map of metadata to store with the object in S3.
* `useAccelerateEndpoint`: whether to use accelerate endpoint.
* `expectedBucketOwner`: the account ID that owns the requested bucket.
* `preventOverwrite`: whether to check if an object with the same key already exists before completing the upload.
* `checksumAlgorithm`: whether to compute the checksum for the data to be uploaded, so the S3 can verify the data integrity.

Note that uploads that were initiated over one hour ago will be cancelled automatically. It is recommended to setup a S3 lifecycle rule to automatically cleanup incomplete upload requests.

To access the S3Client instance for advanced use cases where Amplify does not provide the functionality, you can retrieve the escape hatch. 

For Android, you would use the following Java or Kotlin code to get the escape hatch. 

In Java:
```java
AWSS3StoragePlugin plugin = (AWSS3StoragePlugin) Amplify.Storage.getPlugin("awsS3StoragePlugin");
S3Client client = plugin.getEscapeHatch();
```
In Kotlin:
```kotlin
val plugin = Amplify.Storage.getPlugin("awsS3StoragePlugin") as AWSS3StoragePlugin
val client = plugin.escapeHatch
```
In React, you would use the following code to get the escape hatch:
```javascript
const plugin = Amplify.Storage.getPlugin('awsS3StoragePlugin');
const client = plugin.getEscapeHatch();
```
For iOS, you would use the following Swift code to get the escape hatch:
```swift
do {
    let plugin = try Amplify.Storage.getPlugin(for: "awsS3StoragePlugin")
    guard let storagePlugin = plugin as? AWSS3StoragePlugin else {
        return
    }
    let s3Client = storagePlugin.getEscapeHatch()
    // Make requests using s3Client...
} catch {
    print("Get escape hatch failed with error - \(error)")
}
```
In React, the equivalent code would be:
```javascript
try {
  const plugin = await Amplify.Storage.getPlugin('awsS3StoragePlugin');
  const s3Client = plugin.getEscapeHatch();
  // Make requests using s3Client...
} catch (error) {
  console.log('Get escape hatch failed with error - ', error);
}
```
For additional client documentation and S3Client code examples, see the AWS SDK documentation.

You can use Amplify Storage APIs with your own S3 buckets instead of the ones created by Amplify. 

To do this, you must have Amplify Auth configured in your project. 

First, you need to add necessary permissions to the S3 bucket by going to the Amazon S3 console, selecting the S3 bucket, and editing the bucket policy. 

The policy should look something like this:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "Statement1",
      "Principal": { "AWS": "arn:aws:iam::<AWS-account-ID>:role/<role-name>" },
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::<bucket-name>/*"
      ]
    }
  ]
}
```
Replace `<AWS-account-ID>` with your AWS account ID, `<role-name>` with the IAM role associated with your Amplify Auth setup, and `<bucket-name>` with the S3 bucket name.

Next, you need to specify the S3 bucket in Amplify's backend config by using the `addOutput` method from the backend definition object in your `amplify/backend.ts` file:
```javascript
const backend = defineBackend({
  auth,
  data,
});

backend.addOutput({
  storage: {
    aws_region: "<region>",
    bucket_name: "<bucket-name>"
  },
});
```
Replace `<region>` with the region of the bucket and `<bucket-name>` with the S3 bucket name.

After configuring the necessary permissions, you can start using the storage APIs with your chosen S3 bucket.

If you're not using an Amplify backend, you can still use existing storage resources with Amplify Storage by configuring your storage options manually and ensuring Amplify Auth is properly configured in your project.

You can configure the Amplify Storage client library to interact with the additional resources by passing the resource metadata to `Amplify.configure`:
```typescript
import { Amplify } from 'aws-amplify';

Amplify.configure({
  Auth: {
    // add your auth configuration
  },
  Storage: {
    S3: {
      bucket: '<your-default-bucket-name>',
      region: '<your-default-bucket-region>',
      buckets: {
        '<your-default-bucket-friendly-name>': {
          bucketName: '<your-default-bucket-name>',
          region: '<your-default-bucket-region>'
        },
        '<your-additional-bucket-friendly-name>': {
          bucketName: '<your-additional-bucket-name>',
          region: '<your-additional-bucket-region>'
        }
      }
    }
  }
});
```
Alternatively, you can create or modify the `amplify_outputs.json` file directly:
```json
{
  "auth": {
    // add your auth configuration
  },
  "storage": {
    "aws_region": "<your-default-bucket-region>", 
    "bucket_name": "<your-default-bucket-name>",
    "buckets": [
      {
        "name": "<your-default-bucket-friendly-name>", 
        "bucket_name": "<your-default-bucket-name>", 
        "aws_region": "<your-default-bucket-region>" 
      },
      {
        "name": "<your-additional-bucket-friendly-name>",
        "bucket_name": "<your-additional-bucket-name>",
        "aws_region": "<your-additional-bucket-region>"
      }
    ]
  }
}
```